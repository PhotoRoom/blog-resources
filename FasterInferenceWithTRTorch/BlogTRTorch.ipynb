{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ahead-ecuador",
   "metadata": {},
   "source": [
    "# PyTorch in Production : Faster inference in PyTorch with TRTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "import torch\n",
    "import trtorch\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import torchvision.models as tvm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DEVICE=\"cuda:1\"\n",
    "torch.cuda.set_device(DEVICE)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_resolution(model, resolution, dtype, device):\n",
    "    dummy_input = torch.ones(\n",
    "        (1,3,resolution, resolution),dtype=dtype,device=device\n",
    "    )\n",
    "\n",
    "    # Warm up runs to prepare Cudnn Benchmark\n",
    "    for warm_up_iter in range(10):\n",
    "        prediction = model(dummy_input)\n",
    "    \n",
    "    # Benchmark\n",
    "    with torch.no_grad():\n",
    "        durations = list()\n",
    "        for i in range(100):\n",
    "            start = time.time()\n",
    "            prediction = model(dummy_input)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            durations.append(end-start)\n",
    "    return min(durations)\n",
    "\n",
    "def benchmark(model, resolutions, dtype, device):\n",
    "    results = [benchmark_resolution(model, resolution, dtype, device) for resolution in resolutions]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvm.resnet101(pretrained=True)\n",
    "model.cuda();\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-gregory",
   "metadata": {},
   "source": [
    "## Compile to TRTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-vietnam",
   "metadata": {},
   "source": [
    "#### FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"input_shapes\":[\n",
    "        {\n",
    "            \"min\":[1,3,160,160],\n",
    "            \"opt\":[1,3,160,160],\n",
    "            \"max\":[1,3,1600,1600]\n",
    "        }\n",
    "    ],\n",
    "    \"op_precision\":torch.float\n",
    "}\n",
    "\n",
    "traced_model = torch.jit.trace(model, torch.ones((1,3,160,160), device=DEVICE))\n",
    "float_trt_model = trtorch.compile(traced_model, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-robert",
   "metadata": {},
   "source": [
    "#### FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"input_shapes\":[\n",
    "        {\n",
    "            \"min\":[1,3,160,160],\n",
    "            \"opt\":[1,3,160,160],\n",
    "            \"max\":[1,3,1600,1600]\n",
    "        }\n",
    "    ],\n",
    "    \"op_precision\":torch.half\n",
    "}\n",
    "\n",
    "traced_model = torch.jit.trace(model, torch.ones((1,3,160,160), device=DEVICE))\n",
    "half_trt_model = trtorch.compile(traced_model, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-scroll",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTIONS = [160,224,320,448,640,896,1280,1600]\n",
    "benchmarks = {\n",
    "    \"PyTorch FP32\" : benchmark(\n",
    "        model,\n",
    "        RESOLUTIONS,\n",
    "        dtype=torch.float,\n",
    "        device=DEVICE,\n",
    "    ),\n",
    "    \"PyTorch FP16\" : benchmark(\n",
    "        model.half(),\n",
    "        RESOLUTIONS,\n",
    "        dtype=torch.half,\n",
    "        device=DEVICE,\n",
    "    ),\n",
    "    \"TRTorch FP32\" : benchmark(\n",
    "        float_trt_model,\n",
    "        RESOLUTIONS,\n",
    "        dtype=torch.float,\n",
    "        device=DEVICE,\n",
    "    ),\n",
    "    \"TRTorch FP16\" : benchmark(\n",
    "        half_trt_model,\n",
    "        RESOLUTIONS,\n",
    "        dtype=torch.half,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = benchmarks[\"PyTorch FP32\"] + benchmarks[\"PyTorch FP16\"] + benchmarks[\"TRTorch FP32\"] + benchmarks[\"TRTorch FP16\"]\n",
    "models = [\"PyTorch FP32\"]*len(RESOLUTIONS) + [\"PyTorch FP16\"]*len(RESOLUTIONS)+ [\"TRTorch FP32\"]*len(RESOLUTIONS)+ [\"TRTorch FP16\"]*len(RESOLUTIONS)\n",
    "df = pd.DataFrame(zip(RESOLUTIONS*4, values, models))\n",
    "df.columns = [\"Resolution\", \"Duration (s)\", \"Method\"]\n",
    "df[\"Image Resolution\"] = df[\"Resolution\"].apply(lambda x: RESOLUTIONS.index(x))\n",
    "\n",
    "\n",
    "resolutions_ticks = [f\"{resolution}x{resolution}\" for resolution in RESOLUTIONS]\n",
    "fig = px.bar(\n",
    "    data_frame=df,\n",
    "    x=\"Image Resolution\",\n",
    "    y=\"Duration (s)\",\n",
    "    color=\"Method\",\n",
    "    barmode=\"group\",\n",
    "    title=\"ResNet101 Inference Time\", \n",
    "    height=500,\n",
    "     \n",
    "\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = list(range(len(resolutions_ticks))),\n",
    "        ticktext = resolutions_ticks\n",
    "    ),\n",
    "    bargroupgap=0.,bargap=0.3\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-gazette",
   "metadata": {},
   "source": [
    "### Compute the speedup factor at different resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_METHOD = \"PyTorch FP16\"\n",
    "SECOND_METHOD = \"TRTorch FP16\"\n",
    "\n",
    "\n",
    "first_method_perf = df[df[\"Method\"]==FIRST_METHOD].groupby('Resolution', as_index=False).first()[['Duration (s)']]\n",
    "second_method_perf = df[df[\"Method\"]==SECOND_METHOD].groupby('Resolution', as_index=False).first()[['Duration (s)']]\n",
    "speedup = first_method_perf/ second_method_perf\n",
    "speedup[\"Image Resolution\"] = range(8)\n",
    "speedup.columns = [\"Ratio\", \"Image Resolution\"]\n",
    "\n",
    "fig = px.bar(\n",
    "    data_frame=speedup,\n",
    "    x=\"Image Resolution\",\n",
    "    y=\"Ratio\",\n",
    "    title=f\"Inference Speed-Up Factor : {SECOND_METHOD} vs {FIRST_METHOD}\",\n",
    "\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis = dict(\n",
    "        tickmode = 'array',\n",
    "        tickvals = list(range(8)),\n",
    "        ticktext = resolutions_ticks\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-dylan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
